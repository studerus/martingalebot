---
title: "Introduction to the martingalebot package in R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to the martingalebot package in R}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(martingalebot)
```

The `martingalebot` package provides functions to download
cryptocurrency price data from Binance and to perform backtesting and
parameter optimization for a single pair martingale trading strategy as
implemented by single pair dca bots on [3commas](https://3commas.io/),
[Pionex](https://www.pionex.com/),
[TradeSanta](https://tradesanta.com/), [Mizar](https://mizar.com/),
[OKX](https://www.okx.com/learn/introducing-the-spot-dollar-cost-averaging-dca-bot),
[Bitget](https://www.bitget.com/en/academy/article-details/Bitget-DCA-Strategy-A-Hands-on-Tutorial)
and others.

### Downloading price data

There are three different functions to download data from Binance:
`get_binance_klines(),` `get_binance_klines_from_csv()`,
`get_binance_prices_from_csv`. The function `get_binance_klines()` can
download candlestick data directly. The user can specify the trading
pair, the start and end time and the time frame for the candles. For
example, to download hourly candles from `ETHUSDT` from the first of
January to the first of March 2023, one could specify:

```{r}
get_binance_klines(symbol = 'ETHUSDT',
                   start_time = '2025-01-01',
                   end_time = '2025-03-01',
                   interval = '1h')
```

An advantage of `get_binance_klines()` is that it can download price
data up to the current time. A disadvantage is that the lowest time
frame for the candles is 1 minute.

The function `get_binance_klines_from_csv()` downloads candlestick data
via csv files from <https://data.binance.vision/>. The advantage of this
method is that it is faster for large amounts of data and that that the
lowest time frame for the candles is 1 second. A disadvantage is that it
can only download price data up to 1-2 days ago as the csv files on
[https://data.binance.vision](https://data.binance.vision/) are only
updated once per day.

The function `get_binance_prices_from_csv()` also downloads price data
via csv files from <https://data.binance.vision/> and thus shares the
same advantages and disadvantages, but it downloads aggregated trades
instead of candlestick data. This allows for an even lower time
resolution as it returns all traded prices of a coin over time. Knowing
the exact price at each point in time is particularly helpful for
backtesting martingale bots with trailing buy and sell orders. The
function `get_binance_prices_from_csv()` returns a data frame with only
two columns. See, for example:

```{r}
get_binance_prices_from_csv('LTCBTC',
                            start_time = '2025-01-01',
                            end_time = '2025-02-01', progressbar = F)
```

Since this function returns very large amounts of data for frequently
traded pairs such as `BTCUSDT`, it is, by default, parallelized and
shows a progress bar. Currently, the functions `backtest` and
`grid_search` are implemented in such a way that they expect the price
data to be in the format as returned by this function.


### Visualizing a Martingale Configuration

Before running scans, you can quickly inspect how a parameter set allocates capital across safety orders, where orders are placed, and where the takeâ€‘profit lands.

```{r, dev='svg', out.width='100%', fig.width=8, fig.asp=0.6, fig.align='center'}
# Capital allocation by price level (horizontal stacked bars)
plot_martingale_config(
  starting_price = 100,
  n_safety_orders = 8,
  pricescale = 2.4,
  volumescale = 1.5,
  take_profit = 2.4,
  stepscale = 1.1, 
  plot_type = "allocation"
)
```

```{r, dev='svg', out.width='100%', fig.width=8, fig.asp=0.6, fig.align='center'}
# Timeline view (order sequence with buy amounts and final TP point)
plot_martingale_config(
  starting_price = 100,
  n_safety_orders = 8,
  pricescale = 2.4,
  volumescale = 1.5,
  take_profit = 2.4,
  stepscale = 1.1,
  plot_type = "timeline"
)
```

### Performing a backtest

To perform a backtest of a martingale bot, we first download price data
for a specific time period and trading pair with
`get_binance_prices_from_csv()` and then apply `backtest` to it. The
tested martingale bot can be set up with the following parameters:

-   `base_order_volume`: The size of the base order (in the quote
    currency)

-   `first_safety_order_volume`: The size of the first safety order (in
    the quote currency)

-   `n_safety_orders`: The maximum number of safety orders

-   `pricescale`: Price deviation to open safety orders (% from initial
    order)

-   `volumescale`: With what number should the funds used by the last
    safety order be multiplied?

-   `take_profit`: At what percentage in profit should the bot close the
    deal?

-   `stepscale`: With what number should the price deviation percentage
    used by the last safety order be multiplied.

-   `stoploss`: At what percentage of draw down should a stop-loss be
    triggered? If set to zero (default), a stop-loss will never be
    triggered.

-   `start_asap`: Should new deals be started immediately after the
    previous deal was closed. If set to `FALSE` new deals are only
    started where the logical vector `deal_start` in `data` is `TRUE`.

-   `use_emergency_stop`: Whether to honor an external emergency stop
    signal during backtesting. If set to `TRUE`, the logical column
    `emergency_stop` in `data` (if present) will immediately close any
    open deal when it becomes `TRUE` and prevent new deals while it is
    `TRUE`.

If we don't specify any of these arguments, the default parameter
settings will be used. To show the default settings, type
`args(backtest)` or go to the help file with `?backtest`.

```{r, warning=FALSE}
dat <- get_binance_prices_from_csv('BONKUSDT',
                                   start_time = '2025-03-01',
                                   end_time = '2025-07-01', 
                                   progressbar = F)
dat |> backtest()
```

The backtest function returns the following measures:

-   `profit`: The percentage of profit the bot made during the tested
    time period.

-   `n_trades`: The number of deals (cycles) that have been closed.

-   `max_draw_down`: The biggest draw down in percent hat occurred.

-   `required_capital`: How much capital is needed to run a bot with the
    used parameter settings.

-   `covered_deviation`: The percentage price deviation from the initial
    order to the last safety order.

-   `down_tolerance`: The percentage price deviation from the initial
    order price to the take profit price when all safety orders are used
    up.

-   `max_time`: The maximum number of days the bot was in a stuck
    position (maximum number of days of being fully invested).

-   `percent_inactive`: The percentage of time the bot was in a stuck
    position. That is, all safety orders were filled and the bot was
    fully invested.

-   `n_stoploss`: The number of stop-losses that had been triggered.
 
 -   `n_emergency_stops`: The number of emergency stops that had been triggered
     due to the `emergency_stop` signal when `use_emergency_stop = TRUE`.

If the argument `plot` is `TRUE`, an interactive plot showing the
changes in capital and price of the traded cryptocurrency over time is
produced. Buys, sells and stop-losses are displayed as red, green and
blue dots, respectively.

```{r, include = F}
library(tidyverse)
library(optimization)
library(GA)
dat |> backtest(plot = T)
```

```{r, fig.height=12}
dat |> backtest(plot = T)
```

### Deal start conditions

By default, new trades are started as soon as possible. If the price
data set contains a logical vector `deal_start` and the argument
`start_asap` is set to `FALSE`, new deals are only started where the
logical vector `deal_start` in `data` is `TRUE`. We can add a deal start condition, for
example based on the Relative Strength Index (RSI), by using one of the
`add_*_filter` functions. We can specify the time frame for the candles, the
number of candles that are considered and the cutoff for creating the
logical vector `deal_start`. In the following example, new deals are
only started if the hourly RSI is below 30. You can see in the plot that
there are no buys (red dots) at peaks of the price curve anymore.
However, the performance is slightly worse because there are now less
trades in total.

```{r}
dat |>
  add_rsi_filter(time_period = "1 hour", n = 7, cutoff = 30) |>
  backtest(start_asap = FALSE, plot = TRUE)
```

Other useful deal-start filters provided by the package:

- SMA Trend Filter (trend-following; avoids buying in downtrends). Signals when price is above a long SMA.

```{r}
dat |>
  add_sma_filter(n = 100, column_name = "deal_start", price_is_above = TRUE) |>
  backtest(start_asap = FALSE)
```

- Bollinger %B Oversold (mean reversion; buys local dips within a regime). Triggers when price is near the lower band.

```{r}
dat |>
  add_bollinger_filter(time_period = "1 hour", n = 20, cutoff = 0.10,
                       column_name = "deal_start", signal_on_below = TRUE) |>
  backtest(start_asap = FALSE)
```

- MACD Bullish Crossover (momentum/confirmation; enters on a momentum turn).

```{r}
dat |>
  add_macd_filter(time_period = "4 hours", column_name = "deal_start",
                  macd_is_above_signal = TRUE) |>
  backtest(start_asap = FALSE)
```

- Regime + Entry (robust approach; only buy dips inside a higher-timeframe bull regime). Example: weekly RSI regime AND 4h dip.

```{r}
dat_regime <- dat |>
  add_rsi_filter(time_period = "1 week", n = 14, cutoff = 40,
                 column_name = "is_bull_regime", rsi_is_above = FALSE) |>
  add_rsi_filter(time_period = "4 hours", n = 14, cutoff = 30,
                 column_name = "is_dip", rsi_is_above = FALSE)

dat_regime[, deal_start := is_bull_regime & is_dip]
dat_regime |>
  backtest(start_asap = FALSE)
```


### Emergency Stop Conditions

Emergency stops are rare, high-conviction exit signals to protect the bot from regime changes (e.g., start of a bear market) or extreme momentum down moves. The following helpers produce a logical column named `emergency_stop` that `backtest(..., use_emergency_stop = TRUE)` will honor.

- Weekly RSI breakdown (regime filter; closes deals when long-term momentum turns bearish).

```{r}
dat |>
  add_rsi_filter(time_period = "1 week", n = 14, cutoff = 40,
                 column_name = "emergency_stop", rsi_is_above = FALSE) |>
  backtest(use_emergency_stop = TRUE)
```

- Death Cross (confirmation of trend reversal; short SMA crosses below long SMA on daily data).

```{r}
dat |>
  add_death_cross_filter(column_name = "emergency_stop") |>
  backtest(use_emergency_stop = TRUE)
```

- Rate of Change (ROC) crash (downside momentum; triggers on strong multi-week drawdowns). You can smooth inputs to avoid wick/noise.

```{r}
dat |>
  add_roc_filter(time_period = "1 day", n = 90, cutoff = -30,
                 smoothing_period = 7, column_name = "emergency_stop",
                 roc_is_below = TRUE) |>
  backtest(use_emergency_stop = TRUE)
```

- SMA breakdown (simpler regime guard; closes when price falls below a long SMA).

```{r}
dat |>
  add_sma_filter(n = 200, column_name = "emergency_stop", price_is_above = FALSE) |>
  backtest(use_emergency_stop = TRUE)
```

- Bollinger %B overbought clamp (euphoria exit; stop when price hugs upper band). Useful to avoid buying into blow-off tops or to defensively close.

```{r}
dat |>
  add_bollinger_filter(time_period = "1 day", n = 20, cutoff = 0.95,
                       column_name = "emergency_stop", signal_on_below = FALSE) |>
  backtest(use_emergency_stop = TRUE)
```

Notes:
- Emergency stops should be infrequent; prefer higher timeframes and conservative thresholds.
- You can combine multiple stops by ORâ€‘condition, e.g.`dat[, emergency_stop := stop1 | stop2]`.
- Emergency stops are displayed as purple dots in the plot.

### Parameter optimization

To find the best parameter set for a given time period, we can perform a
grid search using the function `grid_search`. This function takes
possible values of martingale bot parameters, runs the function
`backtest` with each possible combination of these values and returns
the results as a date frame. Each row of this data frame contains the
result of one possible combination of parameters. Since doing a grid
search can be computationally expensive, the `grid_search` function is
parallelized by default.

By default, `grid_search` uses a broad range of parameters. For example,
for `n_safety_orders`, values between 6 and 16 in steps of 2 are tested
(see `args(grid_search)`for default ranges of parameters). However, we
could also use, for, examples, values between 4 and 6, by explicitly
specifying it:

```{r}
res <- dat |> 
  grid_search(n_safety_orders = 4:6, progressbar = F)
res
```

The rows of the returned data frame are ordered by the column `profit`.
In the first row, we see the set of parameters that led to the highest
profit. To plot the best-performing parameter set, we can pass the
values from the first row of `res` as arguments to `backtest` using
`purrr::exec()`. This function takes a function as its first argument
and a list of parameters as its second, which we can create on the fly.

```{r, eval=FALSE}
# First, run the grid search
res <- dat |>
  grid_search(n_safety_orders = 4:6, progressbar = FALSE)

# Then, plot the best result
# We extract the first row as a list of parameters
best_params <- res |> dplyr::slice(1)
# And pass them to backtest using the !!! (big bang) operator
exec(backtest, !!!best_params, data = dat, plot = TRUE)
```

Instead of picking the most profitable parameter constellation, we could
also pick the one with the best compromise between `profit` and
`max_draw_down` by replacing the command `slice(1)` with
`slice_max(profit - max_draw_down)`.

It should be noted that the `grid_search` function also has the
following arguments that allow to restrict the search space:

-   `min_covered_deviation`: the minimum percentage price deviation from
    the initial order to the last safety order a given parameter
    combination must have. Parameter combinations that have a covered
    price deviation less than this value are discarded and not tested.

-   `min_down_tolerance`: the minimum price down tolerance (i.e.
    percentage price deviation from the initial order price to the take
    profit price when all safety orders are filled) a given parameter
    combination must have. Parameter combinations that have a price down
    tolerance less than this value are discarded and not tested.

-   `max_required_capital`: the maximum capital a given parameter
    combination can require. Parameters that require more capital than
    this value are discarded and not tested.

This can be handy because we might only want to search for optimal
parameter combinations within a set of parameters that have minimum
"down tolerance" and thus have certain robustness against sudden price
drops. In this case, it would be a waste of computation time if we
tested all possible combinations of parameters.

Instead of performing a grid search, we can also search for the best
parameter combination with built-in optimization helpers.

### Differential Evolution (DE)

```{r, warning=FALSE}
# Optimize for profit using Differential Evolution
best_de <- de_search(
  data = dat,
  objective_metric = "profit",
  DEoptim_control = list(itermax = 50, NP = 64, trace = FALSE)
)

best_de

# Plot the best configuration found by DE
best_de %>% exec(backtest, !!!., data = dat, plot = TRUE)
```

You can also optimize a custom metric, e.g. a simple risk-adjusted target:

```{r, warning=FALSE}
best_de_custom <- de_search(
  data = dat,
  objective_metric = "profit / (1 + max_draw_down)",
  DEoptim_control = list(itermax = 40, NP = 48, trace = FALSE)
)
best_de_custom
```

### Random Search (Latin Hypercube Sampling)

```{r}
# Random search to explore the space broadly
rand <- random_search(
  data = dat,
  n_samples = 200,
  progressbar = FALSE
)

# Inspect top candidates
rand %>% 
  dplyr::slice_max(profit, n = 5)

# Plot the best one
rand %>%
  dplyr::slice_max(profit, n = 1) %>%
  exec(backtest, !!!., data = dat, plot = TRUE)
```

### Setting Parameter Ranges

You can restrict the search space by providing lower/upper bounds (DE) or ranges (random search).

```{r, warning=F}
# Differential Evolution with custom bounds
best_de_bounds <- de_search(
  data = dat,
  objective_metric = "profit",
  n_safety_orders_bounds = c(6, 14),
  pricescale_bounds      = c(1.2, 3.2),
  volumescale_bounds     = c(1.0, 2.0),
  take_profit_bounds     = c(1.0, 3.0),
  stepscale_bounds       = c(0.8, 1.2),
  stoploss_bounds        = c(0, 40),
  base_order_volume_bounds         = c(10, 50),
  first_safety_order_volume_bounds = c(10, 50),
  DEoptim_control = list(itermax = 40, NP = 48, trace = FALSE)
)
best_de_bounds

# Random search with matching ranges and pre-filters
rand_bounds <- random_search(
  data = dat,
  n_samples = 200,
  n_safety_orders_bounds = c(6, 14),
  pricescale_bounds      = c(1.2, 3.2),
  volumescale_bounds     = c(1.0, 2.0),
  take_profit_bounds     = c(1.0, 3.0),
  stepscale_bounds       = c(0.8, 1.2),
  stoploss_values        = c(0, 25, 30, 40),
  min_covered_deviation  = 8,
  min_down_tolerance     = 8,
  max_required_capital   = 10000,
  progressbar = FALSE
)
rand_bounds %>%
  dplyr::slice_max(profit, n = 1)
```

### Cross-validation

In the previous examples, we used the same data for training and testing
the algorithm. However, this most likely resulted in over-fitting and
over-optimistic performance estimation. A better strategy would be to
strictly separate testing and learning by using cross-validation.

We first download a longer time period of price data so that we have
more data for training and testing:

```{r}
dat <- get_binance_prices_from_csv("ATOMUSDT", 
                                   start_time = '2022-01-01',
                                   end_time = '2023-03-03', progressbar = F)
```

Next, we split our data into many different test and training time
periods. We can use the function `create_timeslices` to create start and
end times of the different splits. It has the following 4 arguments.

-   `train_months` The duration of the training periods in months

-   `test_month` The duration of the testing periods in months

-   `shift_months` The number of months pairs of test and training
    periods are shifted to each other. The smaller this number the more
    pairs of test and training data sets can be created

-   `data` The price data set

For example, if we want to use 4 months for training, 4 months for
testing and create training and testing periods every month, we could
specify:

```{r}
slices <- dat |>
  create_timeslices(train_months = 4, test_months = 4, shift_months = 1)
slices
```

Note that these time periods are partially overlapping. If we want to
have non-overlapping time periods, we could specify `shift_months = 4`.

We can now perform cross-validation by iterating over the rows of
`slices`. At each iteration, we perform a grid search for the best
parameter combination using the training data and then apply this
parameter combination to the test data. For simplicity, we only return
the final performance in the test data.

```{r, message=F}
library(tidyverse)
slices %>% 
  group_by(start_test, end_test) %>% 
  reframe({
    # Get test and training data of the present row / iteration
    train_data <- filter(dat, between(time, start_train, end_train))
    test_data <- filter(dat, between(time, start_test, end_test))
    # Find the best parameter combination in the training data
    best <- train_data |>
      grid_search(progressbar = FALSE) |>
      slice(1)
    # Apply this parameter combination to the test data
    pmap_df(best, backtest, data = test_data)
  })
```

We can see that only 3 of the 7 tested time periods were in profit. This
is because we only maximized profitability during training, which likely
led to the selection of "aggressive" or risky strategies that work well
in the training set but poorly in the test set due to little robustness
against sudden price drops. This is illustrated by the the relatively
small price down tolerance, which varied between 8.2 and 10.9 % for the
selected parameter combinations (see column `down_tolerance` in the
above table). A potential solution to this problem is therefore to
restrict the search space to those parameter combinations that have a
minimum price down tolerance of, for example, 12 %. We can do this by
using the argument `min_down_tolerance` of the `grid_search` function:

```{r, warning=F, message=FALSE}
library(tidyverse)
slices %>% 
  group_by(start_test, end_test) %>% 
  reframe({
    train_data <- filter(dat, between(time, start_train, end_train))
    test_data <- filter(dat, between(time, start_test, end_test))
    best <- train_data |>
      grid_search(min_down_tolerance = 12, progressbar = FALSE) |>
      slice(1)
    pmap_df(best, backtest, data = test_data)
  })
```

Except for the first time period, all time periods are now in profit.
However, this more conservative strategy came with the price of slightly
lower profits in the second and third time periods.

Alternatively, we could also select the most profitable parameter
combination only among those combinations that had little draw down and
did not result in "red bags" for extended periods of time. For example,
to select the most profitable parameter combination among those
combinations that had no more than 30% draw down and that were no longer
than 3% of the time fully invested in the training period, we could do:

```{r, warning=F, message=F}
library(tidyverse)
slices %>% 
  group_by(start_test, end_test) %>% 
  reframe({
    train_data <- filter(dat, between(time, start_train, end_train))
    test_data <- filter(dat, between(time, start_test, end_test))
    best <- train_data |>
      grid_search(progressbar = FALSE) |>
      filter(max_draw_down < 30 & percent_inactive < 3) |>
      slice(1)
    pmap_df(best, backtest, data = test_data)
  })
```

Another option would be to select the parameter combination that
maximizes a combination of measures, such as
`profit - max_draw_down - percent_inactive` .

```{r, warning=F, message=FALSE}
library(tidyverse)
slices %>% 
  group_by(start_test, end_test) %>% 
  reframe({
    train_data <- filter(dat, between(time, start_train, end_train))
    test_data <- filter(dat, between(time, start_test, end_test))
    best <- train_data |>
      grid_search(progressbar = FALSE) |>
      slice_max(profit - max_draw_down - percent_inactive)
    pmap_df(best, backtest, data = test_data)
  })
```

Instead of performing a grid search, we can also run crossâ€‘validation with Differential Evolution (DE) using the builtâ€‘in helper:

```{r, message = F}
library(tidyverse)
slices %>% 
  group_by(start_test, end_test) %>% 
  reframe({
    # Split present fold
    train_data <- filter(dat, between(time, start_train, end_train))
    test_data  <- filter(dat, between(time, start_test,  end_test))

    # Optimize on training set
    best <- de_search(
      data = train_data,
      objective_metric = "profit / (1 + max_draw_down)",
      # keep runtime reasonable for vignette
      DEoptim_control = list(itermax = 30, NP = 48, trace = FALSE)
    )

    # Evaluate on test set
    pmap_df(best, backtest, data = test_data)
  })
```
